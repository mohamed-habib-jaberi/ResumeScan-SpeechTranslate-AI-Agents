{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5216aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Audio folder resolved to: /Users/JABERI/Documents/Dev/AI Agent/KodeKloud/AI Agent/ResumeScan-SpeechTranslate-AI-Agents/Audio Speech Translator AI Agent/audio\n",
      "ğŸ“„ Files found: [PosixPath('/Users/JABERI/Documents/Dev/AI Agent/KodeKloud/AI Agent/ResumeScan-SpeechTranslate-AI-Agents/Audio Speech Translator AI Agent/audio/arabic_sample.mp3'), PosixPath('/Users/JABERI/Documents/Dev/AI Agent/KodeKloud/AI Agent/ResumeScan-SpeechTranslate-AI-Agents/Audio Speech Translator AI Agent/audio/french_sample.mp3'), PosixPath('/Users/JABERI/Documents/Dev/AI Agent/KodeKloud/AI Agent/ResumeScan-SpeechTranslate-AI-Agents/Audio Speech Translator AI Agent/audio/english_sample.mp3')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1ï¸âƒ£ Load API key from .env and init client\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"âŒ OPENAI_API_KEY is missing in .env\")\n",
    "\n",
    "client = AsyncOpenAI(api_key=api_key)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2ï¸âƒ£ Resolve AUDIO_DIR robustly\n",
    "#    Looks for the folder \"Audio Speech Translator AI Agent\"\n",
    "#    and uses its \"audio\" subfolder.\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "AUDIO_FOLDER_NAME = \"Audio Speech Translator AI Agent\"\n",
    "AUDIO_SUBFOLDER = \"audio\"\n",
    "\n",
    "cwd = Path.cwd()\n",
    "project_root = None\n",
    "\n",
    "# Try current dir and all parents\n",
    "for p in [cwd, *cwd.parents]:\n",
    "    if (p / AUDIO_FOLDER_NAME).exists():\n",
    "        project_root = p\n",
    "        break\n",
    "\n",
    "if project_root is None:\n",
    "    # Fallback: assume we're already inside the agent folder\n",
    "    project_root = cwd\n",
    "\n",
    "AUDIO_DIR = project_root / AUDIO_FOLDER_NAME / AUDIO_SUBFOLDER\n",
    "AUDIO_DIR = AUDIO_DIR.resolve()\n",
    "\n",
    "print(f\"ğŸ¯ Audio folder resolved to: {AUDIO_DIR}\")\n",
    "print(\"ğŸ“„ Files found:\", list(AUDIO_DIR.glob(\"*.*\")))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3ï¸âƒ£ Helper: resolve audio path\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_audio_path(file_name: str) -> Path:\n",
    "    name = file_name.strip()\n",
    "    if not name:\n",
    "        raise ValueError(\"âŒ Empty audio filename provided\")\n",
    "\n",
    "    audio_path = (AUDIO_DIR / name).resolve()\n",
    "\n",
    "    if not audio_path.exists():\n",
    "        raise FileNotFoundError(f\"âŒ Audio file not found: {audio_path}\")\n",
    "\n",
    "    return audio_path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4ï¸âƒ£ Transcription with gpt-4o-transcribe\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "async def transcribe_audio(file_path: str) -> str:\n",
    "    audio_path = Path(file_path).resolve()\n",
    "    with open(audio_path, \"rb\") as audio:\n",
    "        transcription = await client.audio.transcriptions.create(\n",
    "            model=\"gpt-4o-transcribe\",\n",
    "            file=audio,\n",
    "        )\n",
    "    return transcription.text\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5ï¸âƒ£ Language + emotion detection\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "async def detect_language_and_emotion(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a JSON dict: { \"language\": \"...\", \"emotion\": \"...\" }\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are an assistant that detects language and emotional tone of a short text.\\n\"\n",
    "        \"Return a JSON object with exactly two keys:\\n\"\n",
    "        \"  - language: the language name in English (e.g., 'French', 'Arabic')\\n\"\n",
    "        \"  - emotion: one or two words describing the dominant emotion (e.g., 'happy', 'sad', 'angry').\\n\\n\"\n",
    "        f\"Text:\\n```{text}```\"\n",
    "    )\n",
    "\n",
    "    resp = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You output only valid JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    content = resp.choices[0].message.content\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback: try to extract something even if not valid JSON\n",
    "        data = {\"language\": \"unknown\", \"emotion\": \"unknown\"}\n",
    "\n",
    "    return {\n",
    "        \"language\": data.get(\"language\", \"unknown\"),\n",
    "        \"emotion\": data.get(\"emotion\", \"unknown\"),\n",
    "    }\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6ï¸âƒ£ Translation\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "async def translate_text(text: str, target_language: str = \"English\") -> str:\n",
    "    \"\"\"\n",
    "    Translate `text` into `target_language`.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Translate the following text into {target_language}. \"\n",
    "        \"Only return the translated text, no explanations.\\n\\n\"\n",
    "        f\"Text:\\n```{text}```\"\n",
    "    )\n",
    "\n",
    "    resp = await client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional translator.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7ï¸âƒ£ Orchestrator: process_audio_translation\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "async def process_audio_translation(file_name: str, target_language: str = \"English\"):\n",
    "    # 0) Resolve audio path\n",
    "    file_path = get_audio_path(file_name)\n",
    "    print(f\"ğŸ§ Using audio file: {file_path}\")\n",
    "\n",
    "    # 1) Transcribe\n",
    "    print(\"ğŸ§ Transcribingâ€¦\")\n",
    "    transcript = await transcribe_audio(str(file_path))\n",
    "    print(f\"ğŸ“ Transcript:\\n{transcript}\\n\")\n",
    "\n",
    "    # 2) Language & emotion\n",
    "    print(\"ğŸ§  Detecting language & emotionâ€¦\")\n",
    "    analysis = await detect_language_and_emotion(transcript)\n",
    "    language = analysis[\"language\"]\n",
    "    emotion = analysis[\"emotion\"]\n",
    "    print(f\"ğŸŒ Detected language: {language}\")\n",
    "    print(f\"ğŸ’“ Detected emotion: {emotion}\\n\")\n",
    "\n",
    "    # 3) Translate\n",
    "    print(f\"ğŸŒ Translating into {target_language}â€¦\")\n",
    "    translated = await translate_text(transcript, target_language=target_language)\n",
    "    print(f\"ğŸ’¬ Translation ({target_language}):\\n{translated}\\n\")\n",
    "\n",
    "    # 4) Return everything as a dict (useful if you want to consume programmatically)\n",
    "    return {\n",
    "        \"file\": str(file_path),\n",
    "        \"transcript\": transcript,\n",
    "        \"language\": language,\n",
    "        \"emotion\": emotion,\n",
    "        \"translation_language\": target_language,\n",
    "        \"translation\": translated,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86711fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ Using audio file: /Users/JABERI/Documents/Dev/AI Agent/KodeKloud/AI Agent/ResumeScan-SpeechTranslate-AI-Agents/Audio Speech Translator AI Agent/audio/french_sample.mp3\n",
      "ğŸ§ Transcribingâ€¦\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************upIA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_audio_translation(\u001b[33m\"\u001b[39m\u001b[33mfrench_sample.mp3\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 144\u001b[39m, in \u001b[36mprocess_audio_translation\u001b[39m\u001b[34m(file_name, target_language)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# 1) Transcribe\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ§ Transcribingâ€¦\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m transcript = \u001b[38;5;28;01mawait\u001b[39;00m transcribe_audio(\u001b[38;5;28mstr\u001b[39m(file_path))\n\u001b[32m    145\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mğŸ“ Transcript:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtranscript\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# 2) Language & emotion\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mtranscribe_audio\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     67\u001b[39m audio_path = Path(file_path).resolve()\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(audio_path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m audio:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     transcription = \u001b[38;5;28;01mawait\u001b[39;00m client.audio.transcriptions.create(\n\u001b[32m     70\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-transcribe\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     71\u001b[39m         file=audio,\n\u001b[32m     72\u001b[39m     )\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m transcription.text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/AI Agent/KodeKloud/AI Agent/ResumeScan-SpeechTranslate-AI-Agents/.venv/lib/python3.14/site-packages/openai/resources/audio/transcriptions.py:908\u001b[39m, in \u001b[36mAsyncTranscriptions.create\u001b[39m\u001b[34m(self, file, model, chunking_strategy, include, known_speaker_names, known_speaker_references, language, prompt, response_format, stream, temperature, timestamp_granularities, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    904\u001b[39m \u001b[38;5;66;03m# It should be noted that the actual Content-Type header that will be\u001b[39;00m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# sent to the server will contain a `boundary` parameter, e.g.\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# multipart/form-data; boundary=---abc--\u001b[39;00m\n\u001b[32m    907\u001b[39m extra_headers = {\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmultipart/form-data\u001b[39m\u001b[33m\"\u001b[39m, **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m    909\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/audio/transcriptions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    910\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m    911\u001b[39m         body,\n\u001b[32m    912\u001b[39m         transcription_create_params.TranscriptionCreateParamsStreaming\n\u001b[32m    913\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m    914\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m transcription_create_params.TranscriptionCreateParamsNonStreaming,\n\u001b[32m    915\u001b[39m     ),\n\u001b[32m    916\u001b[39m     files=files,\n\u001b[32m    917\u001b[39m     options=make_request_options(\n\u001b[32m    918\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m    919\u001b[39m     ),\n\u001b[32m    920\u001b[39m     cast_to=_get_response_format_type(response_format),\n\u001b[32m    921\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    922\u001b[39m     stream_cls=AsyncStream[TranscriptionStreamEvent],\n\u001b[32m    923\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/AI Agent/KodeKloud/AI Agent/ResumeScan-SpeechTranslate-AI-Agents/.venv/lib/python3.14/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Dev/AI Agent/KodeKloud/AI Agent/ResumeScan-SpeechTranslate-AI-Agents/.venv/lib/python3.14/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************upIA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8ï¸âƒ£ Example calls (Notebook usage)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Make sure these files exist in:\n",
    "#   Audio Speech Translator AI Agent/audio/\n",
    "#\n",
    "# await process_audio_translation(\"english_sample.mp3\")\n",
    "# await process_audio_translation(\"french_sample.mp3\")\n",
    "# await process_audio_translation(\"arabic_sample.mp3\")\n",
    "\n",
    "await process_audio_translation(\"french_sample.mp3\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
